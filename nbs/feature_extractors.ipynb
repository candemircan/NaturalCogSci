{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction Functions\n",
    "\n",
    "> Functions to extract features from DNNs.\n",
    "\n",
    "Extracts features for all images written under file_names.txt in the data folder, which are all the images in the THINGS database.\n",
    "\n",
    "You must also have the THINGS images under the stimuli folder in the project root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp feature_extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdemircan/miniconda3/envs/NaturalCogSci/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-09-04 17:28:06.576102: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-04 17:28:15.852965: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-09-04 17:28:15.853687: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-09-04 17:28:15.854346: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (hperry): /proc/driver/nvidia/version does not exist\n",
      "2023-09-04 17:28:16.930401: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/cdemircan/miniconda3/envs/NaturalCogSci/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/home/cdemircan/miniconda3/envs/NaturalCogSci/lib/python3.10/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.10.0 and strictly below 2.13.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.9.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# | exports\n",
    "# | code-fold: false\n",
    "# | output: false\n",
    "import glob\n",
    "import os\n",
    "from os.path import join\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import fasttext\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from thingsvision import get_extractor, get_extractor_from_model\n",
    "from thingsvision.utils.data import ImageDataset, DataLoader\n",
    "import tensorflow_hub as hub\n",
    "import openai\n",
    "from harmonization.models import (\n",
    "    load_ViT_B16,\n",
    "    load_ResNet50,\n",
    "    load_VGG16,\n",
    "    load_EfficientNetB0,\n",
    "    load_tiny_ConvNeXT,\n",
    "    load_tiny_MaxViT,\n",
    "    load_LeViT_small,\n",
    ")\n",
    "from SLIP.models import (\n",
    "    SLIP_VITS16,\n",
    "    SLIP_VITB16,\n",
    "    SLIP_VITL16,\n",
    "    CLIP_VITS16,\n",
    "    CLIP_VITB16,\n",
    "    CLIP_VITL16,\n",
    "    SIMCLR_VITS16,\n",
    "    SIMCLR_VITB16,\n",
    "    SIMCLR_VITL16,\n",
    ")\n",
    "\n",
    "from NaturalCogSci.helpers import get_project_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "\n",
    "\n",
    "def extract_features(\n",
    "    feature_name: str,  # same as model name. In case different encoders are available, it is in `model_encoder` format\n",
    "    use_cached: bool = True,  # If `True`, rerun extraction even if the features are saved. Defaults to True.\n",
    ") -> np.ndarray:  # feature array\n",
    "    \"\"\"\n",
    "    Extract features from a model and save to disk.\n",
    "    \"\"\"\n",
    "    project_root = get_project_root()\n",
    "    temp_feature_path = join(project_root, \"data\", \"temp\", f\"{feature_name}\")\n",
    "    final_feature_path = join(\n",
    "        project_root, \"data\", \"features\", f\"{feature_name.replace('/', '_')}.txt\"\n",
    "    )\n",
    "\n",
    "    hugging_face_dict = {\n",
    "        \"distilbert\": \"distilbert-base-uncased\",\n",
    "        \"bert\": \"bert-base-uncased\",\n",
    "        \"roberta\": \"roberta-base\",\n",
    "    }\n",
    "\n",
    "    if os.path.exists(final_feature_path) and use_cached:\n",
    "        return None\n",
    "\n",
    "    if feature_name == \"task\":\n",
    "        objects = folder_to_word(remove_digit_underscore=False)\n",
    "        ids = pd.read_csv(join(project_root, \"data\", \"THINGS\", \"unique_id.csv\"))[\n",
    "            \"id\"\n",
    "        ].to_list()\n",
    "        weights = np.loadtxt(\n",
    "            join(project_root, \"data\", \"THINGS\", \"spose_embedding_49d_sorted.txt\")\n",
    "        )\n",
    "\n",
    "        features = [weights[ids.index(obj), :] for obj in objects]\n",
    "        features = np.array(features)\n",
    "\n",
    "    elif feature_name == \"ada-002\":\n",
    "        openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        objects = folder_to_word(remove_digit_underscore=True)\n",
    "        objects = [f\"A photo of a {x}\" for x in objects]\n",
    "        features = np.array([get_ada_embedding(obj) for obj in objects])\n",
    "\n",
    "    elif feature_name in [\"bert\", \"roberta\"]:\n",
    "        objects = folder_to_word(remove_digit_underscore=True)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(hugging_face_dict[feature_name])\n",
    "        model = AutoModel.from_pretrained(hugging_face_dict[feature_name]).to(device)\n",
    "        objects = [f\"A photo of a {x}\" for x in objects]\n",
    "\n",
    "        tokenized_objects = tokenizer(\n",
    "            objects, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        tokenized_objects = {\n",
    "            k: torch.tensor(v).to(device) for k, v in tokenized_objects.items()\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            latent_objects = model(**tokenized_objects)\n",
    "\n",
    "        features = latent_objects.last_hidden_state[:, 0, :].numpy()\n",
    "\n",
    "    elif feature_name == \"fasttext\":\n",
    "        objects = folder_to_word(remove_digit_underscore=True)\n",
    "        ft = fasttext.load_model(\n",
    "            join(\n",
    "                project_root,\n",
    "                \"data\",\n",
    "                \"embedding_weights_and_binaries\",\n",
    "                \"crawl-300d-2M-subword.bin\",\n",
    "            )\n",
    "        )\n",
    "        features = [ft.get_word_vector(x) for x in objects]\n",
    "        features = np.array(features)\n",
    "    elif feature_name == \"universal_sentence_encoder\":\n",
    "        objects = folder_to_word(remove_digit_underscore=True)\n",
    "        objects = [f\"A photo of a {x}\" for x in objects]\n",
    "        module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "        model = hub.load(module_url)\n",
    "        features = model(objects).numpy()\n",
    "    else:\n",
    "        features = get_visual_embedding(project_root, feature_name)\n",
    "\n",
    "    feature_name = feature_name.replace(\"/\", \"_\")\n",
    "    np.savetxt(final_feature_path, features)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "def folder_to_word(\n",
    "    remove_digit_underscore: bool,  # Remove digit and underscore from object names if true. Note that you need the digits to get the task embeddings, but not for the others. If True, the underscore gets replaced with a space.\n",
    ") -> list:  # List of object names\n",
    "    \"\"\"\n",
    "    Read file name directories and format them into words by parsing directories\n",
    "    and, on demand, removing any numbers and underscores.\n",
    "    \"\"\"\n",
    "    project_root = get_project_root()\n",
    "    with open(join(project_root, \"data\", \"features\", \"file_names.txt\"), \"r\") as f:\n",
    "        file_names = f.read()[:-1]  # there is an empty line in the end\n",
    "\n",
    "    file_names = file_names.split(\"\\n\")\n",
    "    file_names = [os.path.dirname(x) for x in file_names]\n",
    "    file_names = [os.path.basename(x) for x in file_names]\n",
    "\n",
    "    if remove_digit_underscore:\n",
    "        file_names = [\"\".join([i for i in x if not i.isdigit()]) for x in file_names]\n",
    "        file_names = [x.replace(\"_\", \" \") for x in file_names]\n",
    "    return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "def get_visual_embedding(\n",
    "    project_root: str,  # Root directory of the project\n",
    "    feature_name: str,  # Name of the feature to extract. Must be in `model_config.json`\n",
    ") -> np.ndarray:  # total images by features array\n",
    "    \"\"\"\n",
    "    Extract visual embedding using `thingsvision`\n",
    "    \"\"\"\n",
    "\n",
    "    harmonization_variants = {\n",
    "        \"ViT_B16\": load_ViT_B16,\n",
    "        \"ResNet50\": load_ResNet50,\n",
    "        \"VGG16\": load_VGG16,\n",
    "        \"EfficientNetB0\": load_EfficientNetB0,\n",
    "        \"tiny_ConvNeXT\": load_tiny_ConvNeXT,\n",
    "        \"tiny_MaxViT\": load_tiny_MaxViT,\n",
    "        \"LeViT_small\": load_LeViT_small,\n",
    "    }\n",
    "\n",
    "    slip_variants = {\n",
    "        \"slip_slip_small\": SLIP_VITS16,\n",
    "        \"clip_slip_small\": CLIP_VITS16,\n",
    "        \"simclr_slip_small\": SIMCLR_VITS16,\n",
    "        \"slip_slip_base\": SLIP_VITB16,\n",
    "        \"clip_slip_base\": CLIP_VITB16,\n",
    "        \"simclr_slip_base\": SIMCLR_VITB16,\n",
    "        \"slip_slip_large\": SLIP_VITL16,\n",
    "        \"clip_slip_large\": CLIP_VITL16,\n",
    "        \"simclr_slip_large\": SIMCLR_VITL16,\n",
    "    }\n",
    "\n",
    "    with open(join(project_root, \"data\", \"model_configs.json\")) as f:\n",
    "        file = json.load(f)\n",
    "    model_config = file[feature_name]\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model_parameters = None\n",
    "    save_name = feature_name\n",
    "    if feature_name.startswith(\"Harmonization\"):\n",
    "        extractor = get_extractor_from_model(\n",
    "            model=harmonization_variants[feature_name.split(\"Harmonization_\")[-1]](),\n",
    "            device=device,\n",
    "            backend=\"tf\",\n",
    "        )\n",
    "    elif \"slip\" in feature_name:\n",
    "        weights = torch.load(\n",
    "            join(\n",
    "                project_root,\n",
    "                \"data\",\n",
    "                \"embedding_weights_and_binaries\",\n",
    "                f\"{feature_name}.pth\",\n",
    "            ),\n",
    "            map_location=device,\n",
    "        )\n",
    "\n",
    "        model = slip_variants[feature_name](\n",
    "            ssl_mlp_dim=weights[\"args\"].ssl_mlp_dim,\n",
    "            ssl_emb_dim=weights[\"args\"].ssl_emb_dim,\n",
    "            rand_embed=False,\n",
    "        )\n",
    "\n",
    "        state_dict = OrderedDict()\n",
    "        for k, v in weights[\"state_dict\"].items():\n",
    "            state_dict[k.replace(\"module.\", \"\")] = v\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "\n",
    "        extractor = get_extractor_from_model(\n",
    "            model=model, device=device, backend=\"pt\", forward_fn=model.encode_image\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        if feature_name.startswith(\"clip\"):\n",
    "            model_parameters = {\"variant\": feature_name.split(\"clip_\")[1]}\n",
    "            save_name = feature_name\n",
    "            feature_name = \"clip\"\n",
    "        elif feature_name.startswith(\"OpenCLIP\"):\n",
    "            variant_name = feature_name.split(\"OpenCLIP_\")[1].split(\"_laion\")[0]\n",
    "            dataset_name = feature_name.split(f\"{variant_name}_\")[1]\n",
    "            model_parameters = {\"variant\": variant_name, \"dataset\": dataset_name}\n",
    "            save_name = feature_name\n",
    "            feature_name = \"OpenCLIP\"\n",
    "        elif feature_name.startswith(\"DreamSim\"):\n",
    "            variant_name = feature_name.split(\"DreamSim_\")[1]\n",
    "            model_parameters = {\"variant\": variant_name}\n",
    "            save_name = feature_name\n",
    "            feature_name = \"DreamSim\"\n",
    "\n",
    "        save_name = save_name.replace(\"/\", \"_\")\n",
    "        extractor = get_extractor(\n",
    "            model_name=feature_name,\n",
    "            source=model_config[\"source\"],\n",
    "            device=device,\n",
    "            pretrained=True,\n",
    "            model_parameters=model_parameters,\n",
    "        )\n",
    "\n",
    "    stimuli_path = join(project_root, \"stimuli\")\n",
    "    batch_size = 1\n",
    "\n",
    "    dataset = ImageDataset(\n",
    "        root=stimuli_path,\n",
    "        out_path=join(project_root, \"data\", \"features\"),\n",
    "        backend=extractor.get_backend(),\n",
    "        transforms=extractor.get_transformations(),\n",
    "    )\n",
    "    batches = DataLoader(\n",
    "        dataset=dataset, batch_size=batch_size, backend=extractor.get_backend()\n",
    "    )\n",
    "\n",
    "    flatten_acts = False if feature_name.startswith(\"vit_\") else True\n",
    "\n",
    "    extractor.extract_features(\n",
    "        batches=batches,\n",
    "        module_name=model_config[\"module_name\"],\n",
    "        flatten_acts=flatten_acts,\n",
    "        output_dir=join(project_root, \"data\", \"temp\", save_name),\n",
    "        step_size=1,\n",
    "    )\n",
    "\n",
    "    features = cleanup_temp(project_root, save_name)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "def cleanup_temp(\n",
    "    project_root: str,  # Root directory of the project\n",
    "    save_name: str,  # name of the feature. has to match folder name under temp\n",
    ") -> np.ndarray:  # total images by features array\n",
    "    \"\"\"\n",
    "    Read features for single images from the temp folder.\n",
    "\n",
    "    Combine them into one large array.\n",
    "    \"\"\"\n",
    "\n",
    "    TOTAL_IMAGES = 26107\n",
    "\n",
    "    temp_list = glob.glob(join(project_root, \"data\", \"temp\", save_name, \"*npy\"))\n",
    "    temp_list_sorted = sorted(\n",
    "        temp_list, key=lambda x: int(\"\".join(filter(str.isdigit, x)))\n",
    "    )\n",
    "\n",
    "    # below we index into 0 for generality\n",
    "    # it allows to extract the CLS from pytorch transformers\n",
    "    # while having no effect on other embeddings, which are 1D vectors\n",
    "    feature_array = np.array([np.load(x)[0, :] for x in temp_list_sorted])\n",
    "\n",
    "    assert (\n",
    "        feature_array.shape[0] == TOTAL_IMAGES\n",
    "    ), f\"There are features for only {feature_array.shape[0]} images.\\nIt must be\\\n",
    "        {TOTAL_IMAGES} instead.\\n temp won't be deleted and feature array won't be saved.\"\n",
    "\n",
    "    return feature_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "def get_ada_embedding(\n",
    "    text: str,  # Sentence to be embedded\n",
    "    model: str = \"text-embedding-ada-002\",  # Model to get embeddings from. Defaults to \"text-embedding-ada-002\".\n",
    ") -> np.ndarray:  # word vector\n",
    "    \"\"\"\n",
    "    Generate word embeddings from openai ada model.\n",
    "    \"\"\"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return openai.Embedding.create(input=[text], model=model)[\"data\"][0][\"embedding\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
