{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction Functions\n",
    "\n",
    "> Functions to extract features from DNNs.\n",
    "\n",
    "Extracts features for all images written under file_names.txt in the data folder, which are all the images in the THINGS database.\n",
    "\n",
    "You must also have the THINGS images under the stimuli folder in the project root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp feature_extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "#| code-fold: false\n",
    "#| output: false\n",
    "import glob\n",
    "import os\n",
    "from os.path import join\n",
    "from shutil import rmtree\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import fasttext\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from thingsvision import get_extractor, get_extractor_from_model\n",
    "from thingsvision.utils.data import ImageDataset, DataLoader\n",
    "import tensorflow_hub as hub\n",
    "import openai\n",
    "from harmonization.models import (\n",
    "    load_ViT_B16,\n",
    "    load_ResNet50,\n",
    "    load_VGG16,\n",
    "    load_EfficientNetB0,\n",
    "    load_tiny_ConvNeXT,\n",
    "    load_tiny_MaxViT,\n",
    "    load_LeViT_small,\n",
    ")\n",
    "\n",
    "\n",
    "from NaturalCogSci.helpers import get_project_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "def extract_features(feature_name: str,  # same as model name. In case different encoders are available, it is in `model_encoder` format\n",
    "                     use_cached: bool = True # If `True`, rerun extraction even if the features are saved. Defaults to True.\n",
    "                     )-> np.ndarray: # feature array\n",
    "    \"\"\"\n",
    "    Extract features from a model and save to disk.\n",
    "    \"\"\"\n",
    "    project_root = get_project_root()\n",
    "    temp_feature_path = join(project_root, \"data\", \"temp\", f\"{feature_name}\")\n",
    "    final_feature_path = join(project_root,\"data\",\"features\",f\"{feature_name.replace('/', '_')}.txt\")\n",
    "\n",
    "    hugging_face_dict = {\n",
    "        \"distilbert\": \"distilbert-base-uncased\",\n",
    "        \"bert\": \"bert-base-uncased\",\n",
    "        \"roberta\": \"roberta-base\",\n",
    "    }\n",
    "\n",
    "\n",
    "    if os.path.exists(final_feature_path) and use_cached:\n",
    "        return None\n",
    "\n",
    "    if feature_name == \"task\":\n",
    "        objects = folder_to_word(remove_digit_underscore=False)\n",
    "        ids = pd.read_csv(join(project_root, \"data\", \"THINGS\", \"unique_id.csv\"))[\n",
    "            \"id\"\n",
    "        ].to_list()\n",
    "        weights = np.loadtxt(\n",
    "            join(project_root, \"data\", \"THINGS\", \"spose_embedding_49d_sorted.txt\")\n",
    "        )\n",
    "\n",
    "        features = [weights[ids.index(obj), :] for obj in objects]\n",
    "        features = np.array(features)\n",
    "    \n",
    "    elif feature_name == \"ada-002\":\n",
    "\n",
    "        openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        objects = folder_to_word(remove_digit_underscore=True)\n",
    "        objects = [f\"A photo of a {x}\" for x in objects]\n",
    "        features = np.array([get_ada_embedding(obj) for obj in objects])\n",
    "\n",
    "\n",
    "    elif feature_name in [\"bert\", \"roberta\"]:\n",
    "        objects = folder_to_word(remove_digit_underscore=True)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(hugging_face_dict[feature_name])\n",
    "        model = AutoModel.from_pretrained(hugging_face_dict[feature_name]).to(device)\n",
    "        objects = [f\"A photo of a {x}\" for x in objects]\n",
    "\n",
    "        tokenized_objects = tokenizer(\n",
    "            objects, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        tokenized_objects = {\n",
    "            k: torch.tensor(v).to(device) for k, v in tokenized_objects.items()\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            latent_objects = model(**tokenized_objects)\n",
    "\n",
    "        features = latent_objects.last_hidden_state[:, 0, :].numpy()\n",
    "\n",
    "    elif feature_name == \"fasttext\":\n",
    "        objects = folder_to_word(remove_digit_underscore=True)\n",
    "        ft = fasttext.load_model(\n",
    "            join(project_root, \"data\", \"embedding_weights_and_binaries\", \"crawl-300d-2M-subword.bin\")\n",
    "        )\n",
    "        features = [ft.get_word_vector(x) for x in objects]\n",
    "        features = np.array(features)\n",
    "    elif feature_name == \"universal_sentence_encoder\":\n",
    "        objects = folder_to_word(remove_digit_underscore=True)\n",
    "        objects = [f\"A photo of a {x}\" for x in objects]\n",
    "        module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "        model = hub.load(module_url)\n",
    "        features = model(objects).numpy()\n",
    "    else:\n",
    "        features = get_visual_embedding(project_root, feature_name)\n",
    "\n",
    "\n",
    "    feature_name = feature_name.replace(\"/\", \"_\") \n",
    "    np.savetxt(\n",
    "        final_feature_path, features\n",
    "    )\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def folder_to_word(remove_digit_underscore: bool, # Remove digit and underscore from object names if true. Note that you need the digits to get the task embeddings, but not for the others. If True, the underscore gets replaced with a space.\n",
    "                   ) -> list: # List of object names\n",
    "    \"\"\"\n",
    "    Read file name directories and format them into words by parsing directories\n",
    "    and, on demand, removing any numbers and underscores.\n",
    "    \"\"\"\n",
    "    project_root = get_project_root()\n",
    "    with open(join(project_root, \"data\", \"features\", \"file_names.txt\"), \"r\") as f:\n",
    "        file_names = f.read()[:-1]  # there is an empty line in the end\n",
    "\n",
    "    file_names = file_names.split(\"\\n\")\n",
    "    file_names = [os.path.dirname(x) for x in file_names]\n",
    "    file_names = [os.path.basename(x) for x in file_names]\n",
    "\n",
    "    if remove_digit_underscore:\n",
    "        file_names = [\"\".join([i for i in x if not i.isdigit()]) for x in file_names]\n",
    "        file_names = [x.replace(\"_\", \" \") for x in file_names]\n",
    "    return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def get_visual_embedding(project_root: str, # Root directory of the project\n",
    "                         feature_name: str # Name of the feature to extract. Must be in `model_config.json`\n",
    "                         ) -> np.ndarray: # total images by features array\n",
    "    \"\"\"\n",
    "    Extract visual embedding using `thingsvision`\n",
    "    \"\"\"\n",
    "\n",
    "    harmonization_variants = {\n",
    "    \"ViT_B16\": load_ViT_B16,\n",
    "    \"ResNet50\": load_ResNet50,\n",
    "    \"VGG16\": load_VGG16,\n",
    "    \"EfficientNetB0\": load_EfficientNetB0,\n",
    "    \"tiny_ConvNeXT\": load_tiny_ConvNeXT,\n",
    "    \"tiny_MaxViT\": load_tiny_MaxViT,\n",
    "    \"LeViT_small\": load_LeViT_small,\n",
    "    }\n",
    "\n",
    "    with open(join(project_root, \"data\", \"model_configs.json\")) as f:\n",
    "        file = json.load(f)\n",
    "    model_config = file[feature_name]\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model_parameters = None\n",
    "    save_name = feature_name\n",
    "    if feature_name.startswith(\"Harmonization\"):\n",
    "        extractor = get_extractor_from_model(\n",
    "            model=harmonization_variants[feature_name.split(\"Harmonization_\")[-1]](), \n",
    "            device=device,\n",
    "            backend=\"tf\",\n",
    "            )\n",
    "    else:\n",
    "\n",
    "        if feature_name.startswith(\"clip\"):\n",
    "            model_parameters = {\"variant\": feature_name.split(\"clip_\")[1]}\n",
    "            save_name = feature_name\n",
    "            feature_name = \"clip\"\n",
    "\n",
    "        save_name = save_name.replace(\"/\", \"_\")\n",
    "        extractor = get_extractor(\n",
    "            model_name=feature_name,\n",
    "            source=model_config[\"source\"],\n",
    "            device=device,\n",
    "            pretrained=True,\n",
    "            model_parameters=model_parameters,\n",
    "        )\n",
    "\n",
    "    stimuli_path = join(project_root, \"stimuli\")\n",
    "    batch_size = 1\n",
    "\n",
    "    dataset = ImageDataset(\n",
    "        root=stimuli_path,\n",
    "        out_path=join(project_root, \"data\", \"features\"),\n",
    "        backend=extractor.get_backend(),\n",
    "        transforms=extractor.get_transformations(),\n",
    "    )\n",
    "    batches = DataLoader(\n",
    "        dataset=dataset, batch_size=batch_size, backend=extractor.get_backend()\n",
    "    )\n",
    "\n",
    "    flatten_acts = False if feature_name.startswith(\"vit_\") else True\n",
    "\n",
    "    extractor.extract_features(\n",
    "        batches=batches,\n",
    "        module_name=model_config[\"module_name\"],\n",
    "        flatten_acts=flatten_acts,\n",
    "        output_dir=join(project_root, \"data\", \"temp\", save_name),\n",
    "        step_size=1,\n",
    "    )\n",
    "\n",
    "    \n",
    "    features = cleanup_temp(project_root,save_name)\n",
    "\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def cleanup_temp(project_root:str, # Root directory of the project\n",
    "                 save_name:str # name of the feature. has to match folder name under temp\n",
    "                 ) -> np.ndarray: # total images by features array\n",
    "    \"\"\"\n",
    "    Read features for single images from the temp folder.\n",
    "\n",
    "    Combine them into one large array.\n",
    "\n",
    "    If all the features are available for all images, delete the feature folder\n",
    "    under temp, and return the large array.\n",
    "    \"\"\"\n",
    "\n",
    "    TOTAL_IMAGES = 26107\n",
    "\n",
    "    temp_list = (glob.glob(join(project_root,'data','temp',save_name,'*npy')))\n",
    "    temp_list_sorted = sorted(temp_list,key=lambda x:int(''.join(filter(str.isdigit,x))))\n",
    "\n",
    "    # below we index into 0 for generality\n",
    "    # it allows to extract the CLS from pytorch transformers\n",
    "    # while having no effect on other embeddings, which are 1D vectors\n",
    "    feature_array = np.array([np.load(x)[0,:] for x in temp_list_sorted])\n",
    "\n",
    "    assert feature_array.shape[0] == TOTAL_IMAGES, \\\n",
    "    f\"There are features for only {feature_array.shape[0]} images.\\nIt must be\\\n",
    "        {TOTAL_IMAGES} instead.\\n temp won't be deleted and feature array won't be saved.\"\n",
    "    \n",
    "\n",
    "    return feature_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def get_ada_embedding(text:str, # Sentence to be embedded \n",
    "                      model: str=\"text-embedding-ada-002\" # Model to get embeddings from. Defaults to \"text-embedding-ada-002\".\n",
    "                      ) -> np.ndarray: # word vector\n",
    "    \"\"\"\n",
    "    Generate word embeddings from openai ada model.\n",
    "    \"\"\"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
