{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Read Me!\n",
    "\n",
    "> Code for the project \"Language Aligned Visual Representations Predict Human Behavior in Naturalistic Learning Tasks\"\n",
    "\n",
    "Please see the [paper](https://arxiv.org/abs/2306.09377) for the associated work."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "> Humans possess the ability to identify and generalize relevant features of natural objects, which aids them in various situations. To investigate this phenomenon and determine the most effective representations for predicting human behavior, we conducted two experiments involving category learning and reward learning. Our experiments used realistic images as stimuli, and participants were tasked with making accurate decisions based on novel stimuli for all trials, thereby necessitating generalization. In both tasks, the underlying rules were generated as simple linear functions using stimulus dimensions extracted from human similarity judgments. Notably, participants successfully identified the relevant stimulus features within a few trials, demonstrating effective generalization. We performed an extensive model comparison, evaluating the trial-by-trial predictive accuracy of diverse deep learning models' representations of human choices. Intriguingly, representations from models trained on both text and image data consistently outperformed models trained solely on images, occasionally even surpassing models using the features that generated the task itself. These findings suggest that language-aligned visual representations possess sufficient richness to describe human generalization in naturalistic settings and emphasize the role of language in shaping human cognition."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./task_designs.png){fig-align=\"center\" width=\"100%\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "I ran the analyses on a cluster system that used a SLURM job scheduler and a singularity image. Therefore the bash and slurm scripts are specific to that system. However, the singularity image is available on dockerhub and can be run locally using the following command:\n",
    "\n",
    "```bash\n",
    "docker pull candemircan/naturalcogsci:latest\n",
    "```\n",
    "\n",
    "If you want to use the singularity image, you can pull it as follows\n",
    "\n",
    "```bash\n",
    "singularity pull NaturalCogSci.sif docker://candemircan/naturalcogsci:latest\n",
    "```\n",
    "\n",
    "If you do not want to use containers, you can install the Python dependencies as follows:\n",
    "    \n",
    "```bash\n",
    "git clone https://github.com/candemircan/NaturalCogSci.git\n",
    "cd NaturalCogSci\n",
    "pip install -r requirements.txt\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "For the versions of R dependencies, you can see the `DOCKERFILE`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Variables\n",
    "\n",
    "The code uses the environment variable `NATURALCOGSCI_ROOT` to determine the root directory of the project. You can set this variable in your `.bashrc` file (or whatever your shell rc file might be) as follows:\n",
    "\n",
    "```bash\n",
    "export NATURALCOGSCI_ROOT=/path/to/NaturalCogSci\n",
    "```\n",
    "\n",
    "For me, R could not read this variable from the shell, so I had to set it in the `~/.Renviron` file as well as follows:\n",
    "\n",
    "```bash\n",
    "NATURALCOGSCI_ROOT=/path/to/NaturalCogSci\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "Both experiments are shared under the `experiments` folder. See the `README.md` files in the respective folders for more information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stimuli\n",
    "\n",
    "If you want to extract the features from the images , you need to download the THINGS database under the `stimuli` folder.\n",
    "\n",
    "This can be done from the following link: \n",
    "\n",
    "<https://things-initiative.org/uploads/THINGS/images.zip>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The data is shared in an OSF repository. It should be put under the `data` folder, if you want to use the code as is. All the behavioural (anaonymised) and modelling data can be found in the OSF repo. Further detail about the data are found under the `README.md` file in the `data` folder.\t\n",
    "\n",
    "The OSF repository is here: [https://osf.io/h3t52/](https://osf.io/h3t52/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "\n",
    "If you use our work, please cite our [paper](https://arxiv.org/abs/2306.09377) as such:\n",
    "\n",
    "```bibtex\n",
    "@misc{demircan2023language,\n",
    "      title={Language Aligned Visual Representations Predict Human Behavior in Naturalistic Learning Tasks}, \n",
    "      author={Can Demircan and Tankred Saanum and Leonardo Pettini and Marcel Binz and Blazej M Baczkowski and Paula Kaanders and Christian F Doeller and Mona M Garvert and Eric Schulz},\n",
    "      year={2023},\n",
    "      eprint={2306.09377},\n",
    "      archivePrefix={arXiv},\n",
    "      primaryClass={cs.LG}\n",
    "}\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
